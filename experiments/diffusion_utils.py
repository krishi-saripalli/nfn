import torch
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor
from experiments.siren_utils import unprocess_img_arr
from PIL import Image
from nfn.common.data import params_to_func_params

class EmbeddingData(Dataset):
    def __init__(self, filepath, split='train'):
        """
        Loads SIREN Embeddings from a .pt file generated by make_latent_dset.py
        
        Args:
            filepath (str): Path to the .pt file containing embeddings and labels
            split (str): One of 'train', 'val', or 'test' to determine which data split to use
        """
        # Load the saved data
        data = torch.load(filepath)
        self.embeddings = data['embeddings']
        self.labels = data['labels']
        self.split_points = data['split_points']
        
        # Set up indices based on split points
        if split == 'train':
            self.start_idx = 0
            self.end_idx = self.split_points[0]
        elif split == 'val':
            self.start_idx = self.split_points[0]
            self.end_idx = self.split_points[1]
        else:
            raise ValueError("Split must be one of 'train', 'val'")
        
        self.embeddings = self.embeddings[self.start_idx:self.end_idx]
        self.labels = self.labels[self.start_idx:self.end_idx]
    
    def __len__(self):
        """Returns the number of items in the dataset"""
        return len(self.embeddings)
    
    def __getitem__(self, idx):
        """
        Returns a single item from the dataset
        
        Args:
            idx (int): Index of the item to return
            
        Returns:
            tuple: (embedding, label) pair
        """
        # Return [B, 1, H, W] instead of [B, H, W] because UNetModelWrapper expects dim to be (C,H,W)
        return self.embeddings[idx].unsqueeze(0), self.labels[idx]
    

def decode_latents(nfnet, latents):
    """Helper function to decode latents through the decoder and evaluate SIREN. Returns a list of image tensors"

    Args:
        nfnet (NFNet): A trained NFNet model
        latents (torch.Tensor): A tensor of latents to decode. Should be of shape [batch_size, num_latents, latent_dim]

    Returns:
        list: A list of image tensors
    """
    with torch.no_grad():
        if nfnet.spatial:
            bs, num_latents = latents.shape[:2]
            latents = torch.flatten(latents, end_dim=1)
        
        out = nfnet.decoder(latents)
        
        if nfnet.spatial:
            def unflatten(arr):
                return arr.reshape(bs, num_latents, *arr.shape[1:]).squeeze(2)
            out = out.map(unflatten)
        
        out = params_to_func_params(out)
        
        out = nfnet.batch_siren(out)

        # convert to numpy images
        img_batch = unprocess_img_arr(out.cpu().numpy())
        img_tensors = [Image.fromarray(x) for x in img_batch] # x in img_batch are probably 8-bit int images, which is why we NFN uses PIL fromarray

        img_tensors = [ToTensor()(img) for img in img_tensors]
        return img_tensors
        
